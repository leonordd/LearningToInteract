# LearningToInteract
Este repositÃ³rio serve como meio de disseminaÃ§Ã£o do projeto de dissertaÃ§Ã£o intitulado "Aprender a Interagir".

## Resumo do projeto

O domÃ­nio de investigaÃ§Ã£o em InteraÃ§Ã£o Humano-Computador (IHC) Ã© vasto, no entanto, o conhecimento cientÃ­fico sobre o uso de tÃ©cnicas de InteligÃªncia Artificial (IA) para criar novas formas de interaÃ§Ã£o com os sistemas estÃ¡ ainda pouco desenvolvido. Estes sistemas nÃ£o se adaptam ao ambiente e nÃ£o alteram as suas qualidades ao longo do tempo. Esta investigaÃ§Ã£o propÃµe estudar a aplicabilidade de tÃ©cnicas de aprendizagem computacional na interaÃ§Ã£o com os sistemas. Pretende-se desenvolver um artefacto visual dinÃ¢mico e interativo que aprenda a agir consoante os dados de entrada escolhidos e fornecidos pelo utilizador, fomentando a exploraÃ§Ã£o lÃºdica. Para tal, procedeu-se Ã  revisÃ£o do estado da arte relativo a interfaces tecnolÃ³gicas e Arte, a par da IA. Tendo em mente a fase de investigaÃ§Ã£o, procedeu-se Ã  conceptualizaÃ§Ã£o e implementaÃ§Ã£o do artefacto. O artefacto foi desenvolvido em dois segmentos: artefacto nÃ£o adaptativo â€“ com interatividade com rato e teclado ou gestos predefinidos â€“ e o artefacto adaptativo â€“ com inputs gestuais escolhidos pelo utilizador. Neste Ãºltimo segmento, aplicaram-se e treinaram-se modelos de IA para a personalizaÃ§Ã£o da interaÃ§Ã£o. A componente visual compreendeu o design e desenvolvimento das animaÃ§Ãµes e de outros elementos necessÃ¡rios Ã  representaÃ§Ã£o do artefacto. A implementaÃ§Ã£o seguiu uma abordagem iterativa, que envolveu experiÃªncias e testes diferentes do sistema computacional. Posteriormente, realizaram-se testes de avaliaÃ§Ã£o com utilizadores, o que resultou na discussÃ£o sobre os mesmos. Os resultados obtidos evidenciaram potencialidades e limitaÃ§Ãµes do projeto, que servem de base para uma reflexÃ£o sobre os contributos e possÃ­veis caminhos de investigaÃ§Ã£o futura.

# ImplementaÃ§Ã£o do Brinquedo

## InstalaÃ§Ã£o das dependÃªncias necessÃ¡rias
### Python
Para instalar as dependÃªncias de python:
```bash
pip3 install -r requirements.txt
```
### Processing
Para instalar as dependÃªncias de processing manualmente

COMO INSTALAR:
1. Abrir Processing IDE
2. Ir a Sketch > Import Library > Add Library...
3. Procurar e instalar as seguintes bibliotecas:

```plaintext
1. VIDEO EXPORT
   Nome: Video Export
   Autor: Abe Pazos (hamoid)
   DescriÃ§Ã£o: Para exportar vÃ­deos (com.hamoid.*)
   
   OU pesquisar por: "hamoid" na biblioteca
```

BIBLIOTECAS JÃ INCLUÃDAS (NÃƒO INSTALAR)
As seguintes sÃ£o built-in do Processing/Java:
- javax.swing.* (interface grÃ¡fica)
- java.io.* (input/output de ficheiros)
- java.net.* (networking)
- java.util.concurrent.* (threading)
- java.text.* (formataÃ§Ã£o de texto/datas)

VERIFICAÃ‡ÃƒO â€“ ApÃ³s instalaÃ§Ã£o, verifica se pode usar: import com.hamoid.*;

> âš ï¸ **Nota:** Se nÃ£o encontrar "Video Export", pode tentar: 1. Descarregar diretamente do GitHub: https://github.com/hamoid/video_export_processing; 2. Colocar na pasta libraries do seu sketchbook do Processing




# Treino de Modelo de ClassificaÃ§Ã£o com PyTorch

**Problema de ClassificaÃ§Ã£o**
Este projeto consiste no treino de um modelo de multiclassificaÃ§Ã£o que utiliza PyTorch, a partir de um dataset de inputs extraÃ­do de um ficheiro `.csv`. 

> âš ï¸ **Nota:** Esta versÃ£o foi extraÃ­da do Google Colab (version2.ipyn) e **nÃ£o inclui** a integraÃ§Ã£o entre as partes do MÃ³dulo III. Inclui apenas o treino do modelo.

---

## ğŸ“Œ ReferÃªncias Utilizadas

1. [Build your first ML model in Python (YouTube)](https://www.youtube.com/watch?v=29ZQ3TDGgRQ) | [Google Colab](https://colab.research.google.com/drive/1KDqZvbLXXc75TchFzWmuT43Qq4488HgN)
2. [Train Test Split with Python Machine Learning (Scikit-Learn)](https://www.youtube.com/watch?v=SjOfbbfI2qY)
3. [How to Train a Machine Learning Model with Python](https://www.youtube.com/watch?v=T1nSZWAksNA)
4. [Machine Learning Tutorial Python - 7](https://www.youtube.com/watch?v=fwY9Qv96DJY)
5. [Pytorch Multiclass Classification with ROC and AUC](https://www.youtube.com/watch?v=EoqXQTT74vY) | [GitHub](https://github.com/jeffheaton/app_deep_learning)

---

## ğŸ“‚ Estrutura do cÃ³digo de treino

```plaintext
ğŸ“ data/
   â””â”€â”€ ğŸ“ 0base/
   |   â””â”€â”€ ğŸ“„ dados_teclas1.csv    # Conjunto de dados de saÃ­da
   |   â””â”€â”€ ğŸ“„ video1.mp4           # VÃ­deo do brinquedo
   â””â”€â”€ ğŸ“ dataset43/
       â””â”€â”€ ğŸ“ output/              # ficheiros do modelo treinado
       |     â””â”€â”€ ğŸŒ„ confusion_matrix.png   
       |     â””â”€â”€ ğŸŒ„ training_curves.png   
       |     â””â”€â”€ training_model.pth   
       |     â””â”€â”€ training_statistics.json   
       â””â”€â”€ ğŸ“„ combinado.csv    # Conjunto de dados combinado
       â””â”€â”€ ğŸ“„ v1.csv           # Conjunto de dados de entrada
       â””â”€â”€ ğŸ“„ v1.mp4           # VÃ­deo de captura de gestos

ğŸ“ toy2/          # programa de Processing
ğŸ“ training/      # treino do modelo

ğŸ“„ README.md                   # Este ficheiro
```

---

## âš™ï¸ Tecnologias Usadas
- Python 3.8+
- PyTorch
- pandas / NumPy
- scikit-learn
- Matplotlib / seaborn

---

## ğŸ§  LÃ³gica do CÃ³digo (versÃ£o _coordinates only_)

### 1) **Carregar os dados**
- LÃª `data/<dataset_folder>/combinado.csv`.
- Exclui **todas** as features de visibilidade e colunas nÃ£o-feature:  
  `MillisSinceEpoch`, `TempoVideo`, `AnimacaoAtual`, `ValorMapeado`, `Valor`, `FrameNumber`, `Face`, `Pose`, `RightHand`, `LeftHand`.
- MantÃ©m **apenas as coordenadas** (pose + mÃ£os em x/y/z se existirem como coordenadas, mas **sem** visibilidade).
- `X` = features de coordenadas; `y` = `AnimacaoAtual`.

> âš ï¸ AlteraÃ§Ã£o face Ã  versÃ£o anterior: jÃ¡ **nÃ£o** se usa `Valor (0â€“20)`.  
> Agora o alvo Ã© `AnimacaoAtual` (tipicamente **4 classes: 0â€“3**).

### 2) **PreparaÃ§Ã£o dos dados**
- Substitui valores default `500.0` por `0.0`.
- ConversÃ£o para tensores PyTorch.
- _Split_: treino/validaÃ§Ã£o/teste estratificado.  
  - 1Âº split: `train_test_split(..., test_size=0.3, stratify=y)`  
  - 2Âº split (val/test) aplicado conforme o script.

### 3) **Modelo PyTorch**
- `WeightedFlexibleModel(input_size, output_size, hidden_layers=[256,128,64])`
- AtivaÃ§Ã£o **ReLU** + **Dropout 0.2** nas camadas escondidas.
- **Loss**: `CrossEntropyLoss`  
- **Optimizer**: `Adam(lr=0.0016, weight_decay=1e-4)`  
- **Scheduler**: `ReduceLROnPlateau(patience=10, factor=0.5)`

### 4) **Treino**
- **Default de Ã©pocas**: **10** (alterado do antigo 500).  
- Acompanha `loss`, `accuracy`, `weighted_accuracy`, `grad_norm`, `lr`.  
- **Early stopping** por _weighted accuracy_ de validaÃ§Ã£o (`patience=100` no cÃ³digo atual).
- Guarda o melhor estado do modelo (com base na _weighted accuracy_).

### 5) **AvaliaÃ§Ã£o**
- MÃ©tricas detalhadas (accuracy, precision/recall/F1 macro e por classe, Cohenâ€™s Kappa, MCC).
- **Matriz de confusÃ£o (normalizada)**.
- Curvas de treino (loss/accuracy/lr/grad_norm).

### 6) **Guardar resultados**
- Modelo: `data/<dataset_folder>/output/trained_model.pth`
- EstatÃ­sticas (JSON): `data/<dataset_folder>/output/training_statistics.json`
- GrÃ¡ficos:  
  `training_curves.png` e `confusion_matrix.png` em `data/<dataset_folder>/output/`

---

## ğŸ Como Executar

### 1) Clonar (quando tiver repositÃ³rio)
```bash
git clone https://github.com/ # (ainda sem repo)
cd learning_to_interact/3integration/classification/version23
```

### 2) Instalar dependÃªncias
Se ainda nÃ£o as tiver
```bash
pip install torch pandas numpy scikit-learn matplotlib seaborn joblib
# ou
pip3 install torch pandas numpy scikit-learn matplotlib seaborn joblib
```

### 3) Definir dataset
No topo do script ajustar:
```python
dataset_folder = "dataset48"   # muda conforme a pasta activa em data/
csv_file = "combinado.csv"
```

### 4) Correr o treino
```bash
python 2model_training.py
# ou
python3 2model_training.py
```

---

## ğŸ’¾ Guardar / Carregar Modelo

```python
# Guardar (jÃ¡ feito no fim do script com info completa do treino)
torch.save(model_info, 'data/<dataset_folder>/output/trained_model.pth')

# Carregar (CPU-safe)
checkpoint = torch.load(
    'data/<dataset_folder>/output/trained_model.pth',
    map_location='cpu',
    weights_only=True
)
```

---

## ğŸ§ª ImpressÃµes Ãºteis (no fim do script)
- NÂº de classes e distribuiÃ§Ã£o.
- Arquitetura final e nÂº de parÃ¢metros.
- DimensÃµes dos tensores/logits.
- SumÃ¡rio das mÃ©tricas (test/train), tempo por Ã©poca e total.


## ğŸ” Mapa de ficheiros (versÃ£o atual)

**AquisiÃ§Ã£o / Base** `0fullbody_rec.py`

**PrÃ©-processamento** `1combine_files.py` 

**Modelo / Treino (versÃ£o coordinates-only)** `2model_training.py` 

- **SEM features de visibilidade** (Face, Pose, RightHand, LeftHand)  
- **Treino apenas com coordenadas**  
- **Ã‰pocas por defeito: 10**

**Legacy / ReferÃªncia** `learning_to_interact/toy2/0input_pred.py` 



## ğŸ“Œ Notas rÃ¡pidas
- â€œCoordinates onlyâ€ = exclui todas as colunas de **visibilidade** e as colunas administrativas nÃ£o-feature.  
- Alvo Ã© `AnimacaoAtual` (normalmente 4 classes: `0â€“3`).  
- Valores default `500.0` sÃ£o convertidos para `0.0`.  
- Os artefactos (modelo/JSON/grÃ¡ficos) ficam em: `data/<dataset_folder>/output/`.
